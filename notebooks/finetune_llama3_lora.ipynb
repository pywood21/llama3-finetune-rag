{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2695d95d",
   "metadata": {},
   "source": [
    "Evironment & insatlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed (run only once per environment).\n",
    "# !pip install -U transformers accelerate peft datasets bitsandbytes tensorboard pynvml huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354eb861",
   "metadata": {},
   "source": [
    "Imports & reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5572c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import PeftModel  # imported for potential resume/load checks\n",
    "\n",
    "# Local modules\n",
    "from src.config import ModelConfig, LoRAConfigLite, DataConfig, TrainConfig\n",
    "from src.lora_setup import apply_lora\n",
    "from src.data import load_and_prepare_dataset\n",
    "from src.gpu_utils import print_cuda_info\n",
    "\n",
    "# Enable TF32 on Ampere+ for faster matmul where safe\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f43288",
   "metadata": {},
   "source": [
    "Login (optional) and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2500e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hugging Face login for gated models or pushing checkpoints.\n",
    "# Do not hardcode tokens. Use getpass or environment variables.\n",
    "# from getpass import getpass\n",
    "# login(token=getpass(\"Enter your HF token: \"))\n",
    "\n",
    "model_cfg = ModelConfig()\n",
    "lora_cfg = LoRAConfigLite()\n",
    "data_cfg = DataConfig(\n",
    "    train_path=\"<PATH-TO>/finetune_data.jsonl\",  # <-- replace with your dataset path\n",
    "    max_length=1024,\n",
    "    test_size=0.1,\n",
    "    seed=27,\n",
    "    num_proc=None,  # set e.g., 4 on Linux; None recommended on Windows/Jupyter\n",
    ")\n",
    "train_cfg = TrainConfig(\n",
    "    output_dir=\"outputs/llama3_finetuned\",\n",
    "    logging_dir=\"outputs/logs\"\n",
    ")\n",
    "\n",
    "os.makedirs(train_cfg.output_dir, exist_ok=True)\n",
    "os.makedirs(train_cfg.logging_dir, exist_ok=True)\n",
    "\n",
    "set_seed(train_cfg.seed)\n",
    "print_cuda_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686871b",
   "metadata": {},
   "source": [
    "Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer with safe defaults.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_cfg.model_id,\n",
    "    trust_remote_code=model_cfg.trust_remote_code,\n",
    "    use_fast=True,\n",
    ")\n",
    "# Ensure padding and special tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = model_cfg.padding_side\n",
    "print(\"Tokenizer loaded. pad_token:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5bb10",
   "metadata": {},
   "source": [
    "Load base model in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b08adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "compute_dtype = torch.float16 if model_cfg.bnb_4bit_compute_dtype == \"float16\" else torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=model_cfg.load_in_4bit,\n",
    "    bnb_4bit_use_double_quant=model_cfg.bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=model_cfg.bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# Load model with device_map=\"auto\" for efficient placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_cfg.model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=model_cfg.device_map,\n",
    "    use_safetensors=model_cfg.use_safetensors,\n",
    "    low_cpu_mem_usage=model_cfg.low_cpu_mem_usage,\n",
    "    trust_remote_code=model_cfg.trust_remote_code,\n",
    ")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad48952",
   "metadata": {},
   "source": [
    "Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddee7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model with LoRA adapters\n",
    "model = apply_lora(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8af76",
   "metadata": {},
   "source": [
    "Load & tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb018a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset file should be a JSONL with fields: instruction, input (optional), output.\n",
    "# Example record:\n",
    "# {\"instruction\": \"Translate to English\", \"input\": \"안녕하세요\", \"output\": \"Hello.\"}\n",
    "\n",
    "dataset_dict = load_and_prepare_dataset(data_cfg, tokenizer)\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "eval_dataset = dataset_dict[\"test\"]\n",
    "print(\"Train samples:\", len(train_dataset), \"Eval samples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2c5fa",
   "metadata": {},
   "source": [
    "Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LM collator with causal masking; padding to multiple of 8 helps Tensor Cores.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f0bc6",
   "metadata": {},
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=train_cfg.output_dir,\n",
    "    per_device_train_batch_size=train_cfg.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=train_cfg.gradient_accumulation_steps,\n",
    "    num_train_epochs=train_cfg.num_train_epochs,\n",
    "    logging_steps=train_cfg.logging_steps,\n",
    "    save_steps=train_cfg.save_steps,\n",
    "    eval_steps=train_cfg.eval_steps,\n",
    "    save_total_limit=train_cfg.save_total_limit,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=train_cfg.fp16,\n",
    "    learning_rate=train_cfg.learning_rate,\n",
    "    lr_scheduler_type=train_cfg.lr_scheduler_type,\n",
    "    warmup_ratio=train_cfg.warmup_ratio,\n",
    "    weight_decay=train_cfg.weight_decay,\n",
    "    optim=train_cfg.optim,\n",
    "    report_to=train_cfg.report_to,\n",
    "    logging_dir=train_cfg.logging_dir,\n",
    "    load_best_model_at_end=train_cfg.load_best_model_at_end,\n",
    "    metric_for_best_model=train_cfg.metric_for_best_model,\n",
    "    greater_is_better=train_cfg.greater_is_better,\n",
    "    seed=train_cfg.seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cade9",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918766c",
   "metadata": {},
   "source": [
    "Save final artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter + tokenizer locally\n",
    "trainer.save_model(train_cfg.output_dir)  # saves adapter weights for PEFT\n",
    "tokenizer.save_pretrained(train_cfg.output_dir)\n",
    "print(\"Saved to:\", train_cfg.output_dir)\n",
    "\n",
    "# Optional: push to Hub (requires HF login)\n",
    "# trainer.push_to_hub()  # uncomment if you want to push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23224dba",
   "metadata": {},
   "source": [
    "Quick inference smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a short sample to verify the finetuned adapter works.\n",
    "from transformers import TextStreamer\n",
    "\n",
    "model.eval()\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "prompt = (\n",
    "    \"### Instruction:\\nExplain the difference between moisture content and specific gravity in wood.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        streamer=streamer,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d404f4",
   "metadata": {},
   "source": [
    "TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ebc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir outputs/logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4919f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
