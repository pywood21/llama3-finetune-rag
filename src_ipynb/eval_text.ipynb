{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "def cosine_similarity_sbert(refs, cands, model_name: str, device: str):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between reference and candidate texts using a SentenceTransformer.\n",
    "    Returns a list of cosine similarities for aligned ref/cand pairs.\n",
    "    \"\"\"\n",
    "    sbert = SentenceTransformer(model_name, device=device)\n",
    "    emb_ref = sbert.encode(refs, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "    emb_cand = sbert.encode(cands, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "    return np.sum(emb_ref * emb_cand, axis=1).tolist()\n",
    "\n",
    "def compute_bertscore(refs, cands, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Compute BERTScore (Precision/Recall/F1). Uses 'lang' to select the default model.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    P, R, F1 = bertscore(cands, refs, lang=lang, device=device, verbose=False)\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "# ---------- Perplexity helpers ----------\n",
    "@torch.no_grad()\n",
    "def _sequence_nll(model, tokenizer, text: str, max_ctx: int) -> tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Compute total negative log-likelihood (sum of token-level NLL) and token count for one text.\n",
    "    Uses a sliding window to handle long sequences. Returns (nll_sum, token_count).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Tokenize without truncation; we'll chunk manually\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "    if input_ids.numel() == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Use model max context if available; otherwise tokenizer limit or a safe default\n",
    "    context = max_ctx if max_ctx is not None else getattr(model.config, \"max_position_embeddings\", None)\n",
    "    if context is None or context <= 0 or context > 32768:\n",
    "        context = min(getattr(tokenizer, \"model_max_length\", 8192), 8192)\n",
    "    # we need at least 2 tokens to compute next-token loss\n",
    "    window = max(2, min(int(context), input_ids.numel()))\n",
    "\n",
    "    nll_sum = 0.0\n",
    "    tok_count = 0\n",
    "\n",
    "    # Simple sliding windows without overlap for efficiency.\n",
    "    # If you want higher-fidelity context, you can add overlap (stride < window).\n",
    "    for start in range(0, input_ids.numel(), window):\n",
    "        end = min(start + window, input_ids.numel())\n",
    "        ids = input_ids[start:end]\n",
    "\n",
    "        # For causal LM NLL: shift inputs by one and predict next token.\n",
    "        # We compute loss over all positions except the first in the window.\n",
    "        labels = ids.clone()\n",
    "        # Mask the first position so that there is no target for it\n",
    "        labels[0] = -100\n",
    "\n",
    "        ids = ids.unsqueeze(0).to(model.device)\n",
    "        labels = labels.unsqueeze(0).to(model.device)\n",
    "\n",
    "        out = model(input_ids=ids, labels=labels)\n",
    "        # out.loss is mean over non -100 labels; scale back to token count to aggregate properly\n",
    "        valid = (labels != -100).sum().item()\n",
    "        if valid > 0:\n",
    "            nll_sum += float(out.loss.item()) * valid\n",
    "            tok_count += valid\n",
    "\n",
    "    return nll_sum, tok_count\n",
    "\n",
    "def compute_perplexities(model, tokenizer, texts: list[str], max_ctx: int | None = None) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute per-text perplexity = exp(total_nll / total_tokens). Empty/invalid texts get NaN.\n",
    "    \"\"\"\n",
    "    ppls = []\n",
    "    for t in texts:\n",
    "        nll, cnt = _sequence_nll(model, tokenizer, t, max_ctx=max_ctx)\n",
    "        if cnt == 0:\n",
    "            ppls.append(float(\"nan\"))\n",
    "        else:\n",
    "            ppls.append(math.exp(nll / cnt))\n",
    "    return ppls\n",
    "\n",
    "# ---------- Main entry ----------\n",
    "def evaluate_text_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    eval_sbert_name: str,\n",
    "    *,\n",
    "    # Perplexity options:\n",
    "    ppl_model=None,\n",
    "    ppl_tokenizer=None,\n",
    "    ppl_column: str | None = None,\n",
    "    ppl_max_ctx: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate generation quality with:\n",
    "      - CosineSimilarity (SBERT)\n",
    "      - BERTScore (Precision/Recall/F1)\n",
    "      - (Optional) Perplexity using a provided causal LM (on a chosen column)\n",
    "        * Provide ppl_model and ppl_tokenizer (loaded HF model/tokenizer)\n",
    "        * Set ppl_column to \"Answer\" or \"Generated\" (the column to score)\n",
    "        * ppl_max_ctx optionally limits model context when scoring long texts\n",
    "\n",
    "    BLEU and ROUGE-L are intentionally omitted.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in [\"Answer\", \"Generated\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df[\"Answer\"] = df[\"Answer\"].fillna(\"\").astype(str)\n",
    "    df[\"Generated\"] = df[\"Generated\"].fillna(\"\").astype(str)\n",
    "\n",
    "    eval_df = df[(df[\"Generated\"].str.strip() != \"\") & (df[\"Generated\"] != \"[ERROR]\")].copy()\n",
    "    if eval_df.empty:\n",
    "        eval_df = df.copy()\n",
    "\n",
    "    refs = eval_df[\"Answer\"].tolist()\n",
    "    cands = eval_df[\"Generated\"].tolist()\n",
    "\n",
    "    # (1) Cosine similarity\n",
    "    cos_sims = cosine_similarity_sbert(\n",
    "        refs, cands, eval_sbert_name,\n",
    "        device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "    # (2) BERTScore\n",
    "    P, R, F1 = compute_bertscore(refs, cands, lang=\"en\")\n",
    "\n",
    "    # Attach metrics to eval subset\n",
    "    eval_df.loc[:, \"CosineSimilarity\"] = cos_sims\n",
    "    eval_df.loc[:, \"BERTScore_Precision\"] = P\n",
    "    eval_df.loc[:, \"BERTScore_Recall\"] = R\n",
    "    eval_df.loc[:, \"BERTScore_F1\"] = F1\n",
    "\n",
    "    # (3) Optional Perplexity\n",
    "    ppl_col_name = None\n",
    "    if ppl_model is not None and ppl_tokenizer is not None and ppl_column in (\"Answer\", \"Generated\"):\n",
    "        texts = eval_df[ppl_column].tolist()\n",
    "        ppls = compute_perplexities(ppl_model, ppl_tokenizer, texts, max_ctx=ppl_max_ctx)\n",
    "        ppl_col_name = f\"Perplexity_{ppl_column}\"\n",
    "        eval_df.loc[:, ppl_col_name] = ppls\n",
    "\n",
    "    # Merge back to original shape, leaving non-evaluated rows as NaN for metrics\n",
    "    metric_cols = [\"CosineSimilarity\", \"BERTScore_Precision\", \"BERTScore_Recall\", \"BERTScore_F1\"]\n",
    "    if ppl_col_name:\n",
    "        metric_cols.append(ppl_col_name)\n",
    "\n",
    "    out = df.copy()\n",
    "    for col in metric_cols:\n",
    "        out[col] = np.nan\n",
    "    out.loc[eval_df.index, metric_cols] = eval_df[metric_cols].values\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
