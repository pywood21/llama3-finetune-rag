{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e15afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import faiss\n",
    "from numpy.linalg import norm\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    n = norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "def build_corpus(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure required columns exist and build a combined text field.\n",
    "    Expected columns: Title, Abstract, Keywords, Authors, Year, DOI\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in [\"Title\", \"Abstract\", \"Keywords\", \"Authors\", \"Year\", \"DOI\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df[\"text_full\"] = df[\"Title\"].fillna('') + \". \" + df[\"Abstract\"].fillna('') + \" \" + df[\"Keywords\"].fillna('')\n",
    "    return df\n",
    "\n",
    "def chunk_by_tokens(text: str, tok, max_tok=220, stride=60):\n",
    "    ids = tok.encode(text or \"\", add_special_tokens=False)\n",
    "    chunks, step = [], max(1, max_tok - stride)\n",
    "    for start in range(0, len(ids), step):\n",
    "        piece = ids[start:start+max_tok]\n",
    "        if not piece: break\n",
    "        chunks.append(tok.decode(piece))\n",
    "        if start + max_tok >= len(ids): break\n",
    "    return chunks or [text or \"\"]\n",
    "\n",
    "def make_chunks(df_corpus: pd.DataFrame, hf_tok, max_tok=220, stride=60) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for i, r in df_corpus.iterrows():\n",
    "        base = r[\"text_full\"]\n",
    "        chs = chunk_by_tokens(base, hf_tok, max_tok=max_tok, stride=stride)\n",
    "        for j, ch in enumerate(chs):\n",
    "            records.append({\n",
    "                \"doc_id\": i,\n",
    "                \"chunk_id\": j,\n",
    "                \"text\": ch,\n",
    "                \"Title\": r[\"Title\"],\n",
    "                \"Abstract\": r[\"Abstract\"],\n",
    "                \"Keywords\": r[\"Keywords\"],\n",
    "                \"Authors\": r[\"Authors\"],\n",
    "                \"Year\": r[\"Year\"],\n",
    "                \"DOI\": r[\"DOI\"],\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def build_or_load_embeddings(df_chunks: pd.DataFrame, sbert: SentenceTransformer, cache_npy: str):\n",
    "    if cache_npy and os.path.exists(cache_npy):\n",
    "        vecs = np.load(cache_npy)\n",
    "        vecs = l2_normalize(vecs)\n",
    "        return vecs, True\n",
    "    vecs = sbert.encode(df_chunks[\"text\"].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "    vecs = l2_normalize(vecs)\n",
    "    if cache_npy:\n",
    "        os.makedirs(os.path.dirname(cache_npy), exist_ok=True)\n",
    "        try: np.save(cache_npy, vecs)\n",
    "        except Exception: pass\n",
    "    return vecs, False\n",
    "\n",
    "def build_ip_index(vecs: np.ndarray):\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vecs)\n",
    "    return index\n",
    "\n",
    "def _first_author(authors: str) -> str:\n",
    "    if not isinstance(authors, str): return \"\"\n",
    "    s = authors.strip()\n",
    "    if \";\" in s: return s.split(\";\", 1)[0].strip()\n",
    "    if \" and \" in s: return s.split(\" and \", 1)[0].strip()\n",
    "    return s\n",
    "\n",
    "def _ref_header(row: dict) -> str:\n",
    "    fa = _first_author(row.get(\"Authors\", \"\") or \"\")\n",
    "    title = (row.get(\"Title\", \"\") or \"\")[:320]\n",
    "    year = str(row.get(\"Year\", \"\") or \"\").strip()\n",
    "    doi = str(row.get(\"DOI\", \"\") or \"\").strip()\n",
    "    return f\"{fa} — {title} ({year}). DOI: {doi}\"\n",
    "\n",
    "def mmr(query_vec, cand_vecs, cand_idx, k=8, lambda_=0.72):\n",
    "    selected, selected_idx = [], []\n",
    "    sims = (cand_vecs @ query_vec.T).ravel().copy()\n",
    "    mask = np.ones_like(sims, dtype=bool)\n",
    "    while len(selected) < min(k, len(cand_idx)):\n",
    "        if not selected:\n",
    "            i = int(np.argmax(sims))\n",
    "            selected.append(cand_vecs[i]); selected_idx.append(cand_idx[i])\n",
    "            mask[i] = False; sims[i] = -1e9\n",
    "            continue\n",
    "        sel_mat = np.stack(selected, axis=0)\n",
    "        diversity = (cand_vecs @ sel_mat.T).max(axis=1)\n",
    "        mmr_score = lambda_ * sims + (1 - lambda_) * (-diversity)\n",
    "        mmr_score[~mask] = -1e9\n",
    "        i = int(np.argmax(mmr_score))\n",
    "        selected.append(cand_vecs[i]); selected_idx.append(cand_idx[i])\n",
    "        mask[i] = False; sims[i] = -1e9\n",
    "    return selected_idx\n",
    "\n",
    "def rerank_with_crossencoder(query, rows, cross_encoder, batch_size=32):\n",
    "    pairs = [(query, r[\"text\"]) for r in rows]\n",
    "    scores = []\n",
    "    if not pairs: return rows\n",
    "    parts = np.array_split(np.arange(len(pairs)), max(1, (len(pairs) + batch_size - 1) // batch_size))\n",
    "    for idxs in parts:\n",
    "        batch = [pairs[i] for i in idxs]\n",
    "        scores.extend(cross_encoder.predict(batch))\n",
    "    order = np.argsort(-np.array(scores))\n",
    "    return [rows[i] for i in order]\n",
    "\n",
    "def retrieve_rows(query: str,\n",
    "                  faiss_index,\n",
    "                  embeddings: np.ndarray,\n",
    "                  df_chunks: pd.DataFrame,\n",
    "                  sbert: SentenceTransformer,\n",
    "                  hf_tok,\n",
    "                  k_initial=40, k_final=8, mmr_lambda=0.72,\n",
    "                  cross_encoder=None, ce_batch_size=32,\n",
    "                  max_ctx_tokens=1400, title_max_chars=320, excerpt_max_chars=380):\n",
    "    \"\"\"\n",
    "    Returns a list of row dicts (multiple chunks per document possible).\n",
    "    Token budget is estimated using the new reference header + one snippet block.\n",
    "    \"\"\"\n",
    "    qv = sbert.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    sims, idxs = faiss_index.search(qv, k_initial)\n",
    "    idxs = idxs[0]\n",
    "    cand_rows = [df_chunks.iloc[i].to_dict() for i in idxs]\n",
    "    cand_vecs = embeddings[idxs]\n",
    "\n",
    "    mmr_idx = mmr(qv[0], cand_vecs, idxs, k=max(k_final * 3, 12), lambda_=mmr_lambda)\n",
    "    cand_rows = [df_chunks.iloc[i].to_dict() for i in mmr_idx]\n",
    "\n",
    "    if cross_encoder is not None:\n",
    "        cand_rows = rerank_with_crossencoder(query, cand_rows, cross_encoder, batch_size=ce_batch_size)\n",
    "\n",
    "    packed, used = [], 0\n",
    "    for r in cand_rows:\n",
    "        header = _ref_header(r)\n",
    "        excerpt = r.get(\"text\", \"\") or \"\"\n",
    "        if len(excerpt) > excerpt_max_chars:\n",
    "            excerpt = excerpt[:excerpt_max_chars] + \"…\"\n",
    "        block = f\"{header}\\nSnippet: {excerpt}\\n\"\n",
    "        cost = len(hf_tok.encode(block))\n",
    "        if used + cost > max_ctx_tokens:\n",
    "            break\n",
    "        packed.append(r)\n",
    "        used += cost\n",
    "    return packed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
