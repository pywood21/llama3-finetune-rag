{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ebdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk, numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\").strip().lower())\n",
    "\n",
    "def preprocess_en(text: str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return set(lemmas)\n",
    "\n",
    "def synonyms_en(word: str):\n",
    "    syns = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            syns.add(lemma.name().lower())\n",
    "    return syns\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a / (np.linalg.norm(a) + 1e-12)\n",
    "    b = b / (np.linalg.norm(b) + 1e-12)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def semantic_hit(keyword: str, generated: str, sbert: SentenceTransformer, base_thresh=0.60):\n",
    "    kw = normalize(keyword); gt = normalize(generated)\n",
    "    if not kw or not gt: return False\n",
    "    if len(kw) <= 2: return False\n",
    "    v1 = sbert.encode([kw], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    v2 = sbert.encode([gt], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    return cosine(v1, v2) >= base_thresh\n",
    "\n",
    "def covered_ratio(keywords, generated: str, sbert: SentenceTransformer) -> float:\n",
    "    gen_norm = normalize(generated)\n",
    "    if not keywords: return 0.0\n",
    "    covered = 0\n",
    "    gen_lemmas = preprocess_en(gen_norm)\n",
    "    for raw_kw in keywords:\n",
    "        kw_norm = normalize(raw_kw)\n",
    "        if not kw_norm: continue\n",
    "        lex_hit = (kw_norm in gen_norm)\n",
    "        syn_hit = False\n",
    "        kw_lemmas = preprocess_en(kw_norm)\n",
    "        syns = set()\n",
    "        for w in kw_lemmas:\n",
    "            syns |= synonyms_en(w); syns.add(w)\n",
    "        if syns:\n",
    "            syn_lemmas = set()\n",
    "            for s in syns:\n",
    "                s = s.replace(\"_\", \" \")\n",
    "                for tok in nltk.word_tokenize(s):\n",
    "                    syn_lemmas.add(lemmatizer.lemmatize(tok))\n",
    "            syn_hit = len(gen_lemmas & syn_lemmas) > 0\n",
    "        sem_hit = semantic_hit(kw_norm, gen_norm, sbert, base_thresh=0.60)\n",
    "        if lex_hit or syn_hit or sem_hit:\n",
    "            covered += 1\n",
    "    return covered / max(1, len(keywords))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
