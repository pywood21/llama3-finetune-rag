{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, torch\n",
    "from .prompt import build_messages, render_chat_prompt, build_fallback_prompt\n",
    "\n",
    "RESP_SPLIT_RE = re.compile(r\"###\\s*Answer[:]?\\s*\", flags=re.IGNORECASE)\n",
    "\n",
    "def post_clean(raw_text: str) -> str:\n",
    "    parts = RESP_SPLIT_RE.split(raw_text, maxsplit=1)\n",
    "    response = parts[-1] if len(parts) > 1 else raw_text\n",
    "    response = \" \".join(response.strip().split())\n",
    "    return response.strip()\n",
    "\n",
    "def build_gen_kwargs(tokenizer, max_new=512, temperature=0.2, top_p=0.9, repetition_penalty=1.15, do_sample=True):\n",
    "    return dict(\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=do_sample,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id),\n",
    "    )\n",
    "\n",
    "def ensure_ctx_budget(tokenizer, prompt_text: str, model_max_ctx: int, max_new_tokens: int):\n",
    "    ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids[0]\n",
    "    if len(ids) + max_new_tokens > model_max_ctx:\n",
    "        return max(64, model_max_ctx - len(ids) - 16)\n",
    "    return max_new_tokens\n",
    "\n",
    "def answer_query(query: str, model, tokenizer, rows=None, gen_cfg=None):\n",
    "    messages = build_messages(query, rows=rows)\n",
    "    prompt_text = render_chat_prompt(tokenizer, messages) or build_fallback_prompt(query, rows=rows)\n",
    "    max_new = ensure_ctx_budget(tokenizer, prompt_text, gen_cfg.model_max_ctx, gen_cfg.max_new_tokens)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=gen_cfg.model_max_ctx - max_new).to(model.device)\n",
    "    kwargs = build_gen_kwargs(tokenizer, max_new=max_new, temperature=gen_cfg.temperature, top_p=gen_cfg.top_p,\n",
    "                              repetition_penalty=gen_cfg.repetition_penalty, do_sample=gen_cfg.do_sample)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, **kwargs)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return post_clean(text)\n",
    "\n",
    "def safe_generate(query: str, model, tokenizer, retriever_fn, use_rag: bool, gen_cfg, retry_sleep=0.03):\n",
    "    tries = [\n",
    "        dict(use_rag_flag=use_rag, k=3, max_ctx_tokens=getattr(gen_cfg, \"max_ctx_tokens\", 1400), max_new=gen_cfg.max_new_tokens, do_sample=True),\n",
    "        dict(use_rag_flag=use_rag, k=2, max_ctx_tokens=1200, max_new=min(384, gen_cfg.max_new_tokens), do_sample=True),\n",
    "        dict(use_rag_flag=use_rag, k=2, max_ctx_tokens=1200, max_new=min(384, gen_cfg.max_new_tokens), do_sample=False),\n",
    "        dict(use_rag_flag=False, k=0, max_ctx_tokens=0, max_new=320, do_sample=False),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for t in tries:\n",
    "        try:\n",
    "            rows = retriever_fn(query, k_final=t[\"k\"], max_ctx_tokens=t[\"max_ctx_tokens\"]) if t[\"use_rag_flag\"] else None\n",
    "            return answer_query(query, model, tokenizer, rows=rows, gen_cfg=gen_cfg), {\"status\": \"ok\", \"use_rag\": t[\"use_rag_flag\"]}\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "            time.sleep(retry_sleep)\n",
    "    return \"[ERROR]\", {\"status\": \"fail\", \"error\": last_err}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
